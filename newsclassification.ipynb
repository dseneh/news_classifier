{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "from matplotlib import pyplot\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.models import model_from_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, '20_newsgroup')\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "Found 19997 texts.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[name] = label_id\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            if fname.isdigit():\n",
    "                fpath = os.path.join(path, fname)\n",
    "                args = {} if sys.version_info < (3,) else {'encoding': 'latin-1'}\n",
    "                with open(fpath, **args) as f:\n",
    "                    t = f.read()\n",
    "                    i = t.find('\\n\\n')  # skip header\n",
    "                    if 0 < i:\n",
    "                        t = t[i:]\n",
    "                    texts.append(t)\n",
    "                labels.append(label_id)\n",
    "\n",
    "print('Found %s texts.' % len(texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 174074 unique tokens.\n",
      "Shape of data tensor: (19997, 1000)\n",
      "Shape of label tensor: (19997, 20)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/25\n",
      "15998/15998 [==============================] - 61s 4ms/step - loss: 2.5048 - acc: 0.1884 - f1_m: 0.0607 - precision_m: 0.3780 - recall_m: 0.0343 - val_loss: 2.1878 - val_acc: 0.2431 - val_f1_m: 0.0992 - val_precision_m: 0.5843 - val_recall_m: 0.0545\n",
      "Epoch 2/25\n",
      "15998/15998 [==============================] - 66s 4ms/step - loss: 1.6379 - acc: 0.4311 - f1_m: 0.3221 - precision_m: 0.6767 - recall_m: 0.2150 - val_loss: 1.4909 - val_acc: 0.5201 - val_f1_m: 0.3442 - val_precision_m: 0.8269 - val_recall_m: 0.2182\n",
      "Epoch 3/25\n",
      "15998/15998 [==============================] - 66s 4ms/step - loss: 1.2849 - acc: 0.5581 - f1_m: 0.5038 - precision_m: 0.7533 - recall_m: 0.3804 - val_loss: 1.1801 - val_acc: 0.6204 - val_f1_m: 0.5220 - val_precision_m: 0.8635 - val_recall_m: 0.3746\n",
      "Epoch 4/25\n",
      "15998/15998 [==============================] - 66s 4ms/step - loss: 1.0772 - acc: 0.6361 - f1_m: 0.6085 - precision_m: 0.7923 - recall_m: 0.4954 - val_loss: 1.0502 - val_acc: 0.6754 - val_f1_m: 0.5824 - val_precision_m: 0.8816 - val_recall_m: 0.4357\n",
      "Epoch 5/25\n",
      "15998/15998 [==============================] - 67s 4ms/step - loss: 0.9362 - acc: 0.6832 - f1_m: 0.6652 - precision_m: 0.8131 - recall_m: 0.5639 - val_loss: 0.9236 - val_acc: 0.6972 - val_f1_m: 0.6512 - val_precision_m: 0.8766 - val_recall_m: 0.5192\n",
      "Epoch 6/25\n",
      "15998/15998 [==============================] - 65s 4ms/step - loss: 0.8293 - acc: 0.7241 - f1_m: 0.7099 - precision_m: 0.8337 - recall_m: 0.6190 - val_loss: 0.9067 - val_acc: 0.7032 - val_f1_m: 0.6767 - val_precision_m: 0.8637 - val_recall_m: 0.5571\n",
      "Epoch 7/25\n",
      "15998/15998 [==============================] - 66s 4ms/step - loss: 0.7368 - acc: 0.7519 - f1_m: 0.7453 - precision_m: 0.8501 - recall_m: 0.6642 - val_loss: 0.8439 - val_acc: 0.7172 - val_f1_m: 0.6993 - val_precision_m: 0.8621 - val_recall_m: 0.5893\n",
      "Epoch 8/25\n",
      "15998/15998 [==============================] - 66s 4ms/step - loss: 0.6681 - acc: 0.7755 - f1_m: 0.7691 - precision_m: 0.8565 - recall_m: 0.6984 - val_loss: 0.7812 - val_acc: 0.7419 - val_f1_m: 0.7231 - val_precision_m: 0.8628 - val_recall_m: 0.6230\n",
      "Epoch 9/25\n",
      "15998/15998 [==============================] - 66s 4ms/step - loss: 0.6070 - acc: 0.7975 - f1_m: 0.7937 - precision_m: 0.8693 - recall_m: 0.7309 - val_loss: 0.7596 - val_acc: 0.7552 - val_f1_m: 0.7343 - val_precision_m: 0.8644 - val_recall_m: 0.6391\n",
      "Epoch 10/25\n",
      "15998/15998 [==============================] - 66s 4ms/step - loss: 0.5559 - acc: 0.8130 - f1_m: 0.8114 - precision_m: 0.8764 - recall_m: 0.7559 - val_loss: 0.7631 - val_acc: 0.7462 - val_f1_m: 0.7346 - val_precision_m: 0.8409 - val_recall_m: 0.6525\n",
      "Epoch 11/25\n",
      "15998/15998 [==============================] - 65s 4ms/step - loss: 0.4981 - acc: 0.8300 - f1_m: 0.8305 - precision_m: 0.8858 - recall_m: 0.7820 - val_loss: 0.7377 - val_acc: 0.7564 - val_f1_m: 0.7569 - val_precision_m: 0.8473 - val_recall_m: 0.6843\n",
      "Epoch 12/25\n",
      "15998/15998 [==============================] - 66s 4ms/step - loss: 0.4516 - acc: 0.8451 - f1_m: 0.8470 - precision_m: 0.8974 - recall_m: 0.8024 - val_loss: 0.8594 - val_acc: 0.7152 - val_f1_m: 0.7191 - val_precision_m: 0.7844 - val_recall_m: 0.6641\n",
      "Epoch 13/25\n",
      "15998/15998 [==============================] - 66s 4ms/step - loss: 0.4216 - acc: 0.8512 - f1_m: 0.8538 - precision_m: 0.8969 - recall_m: 0.8151 - val_loss: 0.8081 - val_acc: 0.7367 - val_f1_m: 0.7382 - val_precision_m: 0.8103 - val_recall_m: 0.6784\n",
      "Epoch 14/25\n",
      "15998/15998 [==============================] - 66s 4ms/step - loss: 0.3961 - acc: 0.8624 - f1_m: 0.8643 - precision_m: 0.9019 - recall_m: 0.8300 - val_loss: 0.7277 - val_acc: 0.7602 - val_f1_m: 0.7608 - val_precision_m: 0.8360 - val_recall_m: 0.6984\n",
      "Epoch 15/25\n",
      "15998/15998 [==============================] - 66s 4ms/step - loss: 0.3667 - acc: 0.8733 - f1_m: 0.8765 - precision_m: 0.9115 - recall_m: 0.8444 - val_loss: 0.7417 - val_acc: 0.7594 - val_f1_m: 0.7655 - val_precision_m: 0.8301 - val_recall_m: 0.7107\n",
      "Epoch 16/25\n",
      "15998/15998 [==============================] - 66s 4ms/step - loss: 0.3374 - acc: 0.8850 - f1_m: 0.8853 - precision_m: 0.9143 - recall_m: 0.8583 - val_loss: 0.8151 - val_acc: 0.7427 - val_f1_m: 0.7501 - val_precision_m: 0.8065 - val_recall_m: 0.7013\n",
      "Epoch 17/25\n",
      "15998/15998 [==============================] - 66s 4ms/step - loss: 0.3173 - acc: 0.8900 - f1_m: 0.8916 - precision_m: 0.9192 - recall_m: 0.8657 - val_loss: 0.8313 - val_acc: 0.7467 - val_f1_m: 0.7556 - val_precision_m: 0.8068 - val_recall_m: 0.7108\n",
      "Epoch 18/25\n",
      "15998/15998 [==============================] - 66s 4ms/step - loss: 0.3071 - acc: 0.8940 - f1_m: 0.8969 - precision_m: 0.9215 - recall_m: 0.8737 - val_loss: 0.7634 - val_acc: 0.7702 - val_f1_m: 0.7783 - val_precision_m: 0.8273 - val_recall_m: 0.7352\n",
      "Epoch 19/25\n",
      "15998/15998 [==============================] - 65s 4ms/step - loss: 0.2772 - acc: 0.9016 - f1_m: 0.9035 - precision_m: 0.9252 - recall_m: 0.8829 - val_loss: 0.7399 - val_acc: 0.7699 - val_f1_m: 0.7738 - val_precision_m: 0.8237 - val_recall_m: 0.7300\n",
      "Epoch 20/25\n",
      "15998/15998 [==============================] - 66s 4ms/step - loss: 0.2658 - acc: 0.9062 - f1_m: 0.9072 - precision_m: 0.9282 - recall_m: 0.8874 - val_loss: 0.7879 - val_acc: 0.7732 - val_f1_m: 0.7742 - val_precision_m: 0.8148 - val_recall_m: 0.7378\n",
      "Epoch 21/25\n",
      "15998/15998 [==============================] - 66s 4ms/step - loss: 0.2510 - acc: 0.9152 - f1_m: 0.9171 - precision_m: 0.9354 - recall_m: 0.8996 - val_loss: 0.7594 - val_acc: 0.7749 - val_f1_m: 0.7788 - val_precision_m: 0.8187 - val_recall_m: 0.7429\n",
      "Epoch 22/25\n",
      "15998/15998 [==============================] - 66s 4ms/step - loss: 0.2479 - acc: 0.9105 - f1_m: 0.9126 - precision_m: 0.9302 - recall_m: 0.8957 - val_loss: 0.7717 - val_acc: 0.7757 - val_f1_m: 0.7815 - val_precision_m: 0.8184 - val_recall_m: 0.7481\n",
      "Epoch 23/25\n",
      "15998/15998 [==============================] - 66s 4ms/step - loss: 0.2371 - acc: 0.9146 - f1_m: 0.9145 - precision_m: 0.9310 - recall_m: 0.8988 - val_loss: 0.8281 - val_acc: 0.7622 - val_f1_m: 0.7672 - val_precision_m: 0.8048 - val_recall_m: 0.7331\n",
      "Epoch 24/25\n",
      "15998/15998 [==============================] - 66s 4ms/step - loss: 0.2308 - acc: 0.9197 - f1_m: 0.9199 - precision_m: 0.9360 - recall_m: 0.9046 - val_loss: 0.8888 - val_acc: 0.7579 - val_f1_m: 0.7616 - val_precision_m: 0.7962 - val_recall_m: 0.7301\n",
      "Epoch 25/25\n",
      "15998/15998 [==============================] - 66s 4ms/step - loss: 0.2216 - acc: 0.9211 - f1_m: 0.9222 - precision_m: 0.9364 - recall_m: 0.9086 - val_loss: 0.8515 - val_acc: 0.7517 - val_f1_m: 0.7595 - val_precision_m: 0.7925 - val_recall_m: 0.7294\n",
      "Accuracy: 0.7516879439353943\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Training model.')\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = Dropout(0.2)(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc', f1_m, precision_m, recall_m])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=25,\n",
    "          validation_data=(x_val, y_val))\n",
    "scores = model.evaluate(x_val, y_val, verbose=0)\n",
    "print(\"Accuracy:\", scores[1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-421e1a0e9970>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# evaluate loaded model on test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmsprop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s: %.2f%%\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "getn = pickle.load(open('model_predict.plk', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02473072 0.03593963 0.05290436 0.03727063 0.06699166 0.03838674\n",
      "  0.08902645 0.10702742 0.06311402 0.03988323 0.02913406 0.03092566\n",
      "  0.05372551 0.05852738 0.0703401  0.01761333 0.04885638 0.04990239\n",
      "  0.04867645 0.03702385]]\n",
      "Category:  rec.autos\n",
      "text:  good day\n",
      "=====================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rec.autos': 'good day'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news=\"good day\"\n",
    "getn(news)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
