{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "Found 19997 texts.\n",
      "0\n",
      "loading model .....\n",
      "Loaded model from disk\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "from keras.models import model_from_json\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pickle\n",
    "\n",
    "\n",
    "config = tf.ConfigProto(\n",
    "    device_count={'GPU': 1},\n",
    "    intra_op_parallelism_threads=1,\n",
    "    allow_soft_placement=True\n",
    ")\n",
    "\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.6\n",
    "\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "keras.backend.set_session(session)\n",
    "\n",
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, '20_newsgroup')\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "index_to_label_dict = {}\n",
    "labels = []  # list of label ids\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[name] = label_id\n",
    "        index_to_label_dict[label_id] = name\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            if fname.isdigit():\n",
    "                fpath = os.path.join(path, fname)\n",
    "                args = {} if sys.version_info < (3,) else {'encoding': 'latin-1'}\n",
    "                with open(fpath, **args) as f:\n",
    "                    t = f.read()\n",
    "                    i = t.find('\\n\\n')  # skip header\n",
    "                    if 0 < i:\n",
    "                        t = t[i:]\n",
    "                    texts.append(t)\n",
    "                labels.append(label_id)\n",
    "\n",
    "print('Found %s texts.' % len(texts))\n",
    "#print(texts.shape)\n",
    "print(labels[0])\n",
    "\n",
    "print(\"loading model .....\")\n",
    "#set_session(session)\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "loaded_model._make_predict_function()\n",
    "print(\"done\")\n",
    "#finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getNewsClassification(news_data):\n",
    "    \n",
    "     with session.as_default():\n",
    "            with session.graph.as_default():\n",
    "                newsList = []\n",
    "                newsList.append(news_data)\n",
    "                test_sequences = tokenizer.texts_to_sequences(newsList)\n",
    "                test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "                nn_output = loaded_model.predict(test_data)\n",
    "                print(nn_output)\n",
    "                i=0\n",
    "                news_clssification = {}\n",
    "                for idx in np.argmax(nn_output, axis=1):\n",
    "                    print(\"Category: \", index_to_label_dict[idx])\n",
    "                    print(\"text: \" , newsList[i])\n",
    "                    print(\"=====================================\")\n",
    "                    news_clssification[index_to_label_dict[idx]] = newsList[i]\n",
    "                    i = i + 1\n",
    "            return news_clssification\n",
    "pickle.dump(getNewsClassification, open('model_predict.plk', 'wb'), protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "getn = pickle.load(open('model_predict.plk', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02514832 0.02083086 0.04633236 0.02526926 0.05171706 0.06452749\n",
      "  0.01145351 0.02898723 0.09786203 0.01074342 0.00594416 0.0211697\n",
      "  0.03547251 0.37474245 0.06428679 0.00505396 0.02395642 0.00696495\n",
      "  0.02781193 0.05172555]]\n",
      "Category:  sci.med\n",
      "text:  Ther was a severe accident leading many people dead\n",
      "=====================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sci.med': 'Ther was a severe accident leading many people dead'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getn(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02514832 0.02083086 0.04633236 0.02526926 0.05171706 0.06452749\n",
      "  0.01145351 0.02898723 0.09786203 0.01074342 0.00594416 0.0211697\n",
      "  0.03547251 0.37474245 0.06428679 0.00505396 0.02395642 0.00696495\n",
      "  0.02781193 0.05172555]]\n",
      "Category:  sci.med\n",
      "text:  Ther was a severe accident leading many people dead\n",
      "=====================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sci.med': 'Ther was a severe accident leading many people dead'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news=\"Ther was a severe accident leading many people dead\"\n",
    "getNewsClassification(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
